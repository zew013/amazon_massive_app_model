@online{changAdvancedTechniquesFinetuning2021,
  title = {Advanced {{Techniques}} for {{Fine-tuning Transformers}}},
  author = {Chang, Peggy},
  date = {2021-11-18T10:00:23},
  url = {https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e},
  urldate = {2022-11-26},
  abstract = {Learn these advanced techniques and see how they can help improve results},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/xiyan/Zotero/storage/FAEICTE7/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e.html}
}

@misc{chenBERTJointIntent2019,
  title = {{{BERT}} for {{Joint Intent Classification}} and {{Slot Filling}}},
  author = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
  date = {2019-02-28},
  number = {arXiv:1902.10909},
  eprint = {1902.10909},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.10909},
  url = {http://arxiv.org/abs/1902.10909},
  urldate = {2022-11-26},
  abstract = {Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xiyan/Zotero/storage/5LC2FC2D/Chen et al. - 2019 - BERT for Joint Intent Classification and Slot Fill.pdf;/Users/xiyan/Zotero/storage/EUEJI3SB/1902.html}
}

@misc{gaoSimCSESimpleContrastive2022,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  date = {2022-05-18},
  number = {arXiv:2104.08821},
  eprint = {2104.08821},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.08821},
  urldate = {2022-11-26},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xiyan/Zotero/storage/JB5IZZYR/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf}
}

@misc{khoslaSupervisedContrastiveLearning2021,
  title = {Supervised {{Contrastive Learning}}},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  date = {2021-03-10},
  number = {arXiv:2004.11362},
  eprint = {2004.11362},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.11362},
  urldate = {2022-11-26},
  abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/xiyan/Zotero/storage/J6TVJ84K/Khosla et al. - 2021 - Supervised Contrastive Learning.pdf}
}

@misc{larsonEvaluationDatasetIntent2019,
  title = {An {{Evaluation Dataset}} for {{Intent Classification}} and {{Out-of-Scope Prediction}}},
  author = {Larson, Stefan and Mahendran, Anish and Peper, Joseph J. and Clarke, Christopher and Lee, Andrew and Hill, Parker and Kummerfeld, Jonathan K. and Leach, Kevin and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason},
  date = {2019-09-04},
  number = {arXiv:1909.02027},
  eprint = {1909.02027},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.02027},
  url = {http://arxiv.org/abs/1909.02027},
  urldate = {2022-11-26},
  abstract = {Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope---i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xiyan/Zotero/storage/DKLL24IN/Larson et al. - 2019 - An Evaluation Dataset for Intent Classification an.pdf;/Users/xiyan/Zotero/storage/GXT74KLV/1909.html}
}

@misc{wuTODBERTPretrainedNatural2020,
  title = {{{TOD-BERT}}: {{Pre-trained Natural Language Understanding}} for {{Task-Oriented Dialogue}}},
  shorttitle = {{{TOD-BERT}}},
  author = {Wu, Chien-Sheng and Hoi, Steven and Socher, Richard and Xiong, Caiming},
  date = {2020-10-01},
  number = {arXiv:2004.06871},
  eprint = {2004.06871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.06871},
  url = {http://arxiv.org/abs/2004.06871},
  urldate = {2022-11-26},
  abstract = {The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xiyan/Zotero/storage/ETP67HNV/Wu et al. - 2020 - TOD-BERT Pre-trained Natural Language Understandi.pdf;/Users/xiyan/Zotero/storage/2ZE8NZEH/2004.html}
}
